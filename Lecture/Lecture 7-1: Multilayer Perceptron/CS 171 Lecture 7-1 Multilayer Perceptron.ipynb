{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa78574b-d9db-4449-b800-0aa9b1460993",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron\n",
    "In this notebook, we will expand on our implementation of the single layer perceptron to create a *multi*-layer perceptron. This implementation will allow our model to learn more about the varying structures of hand-drawn numbers compared to the single layer implementation.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Implement a multi-layer perceptron for multi-class classifications of multidimensional data.\n",
    "2. Compute the gradient of a multi-dimensional loss function for use in a multi-layer perceptron.\n",
    "3. Implement a multi-layer perceptron in a Python class.\n",
    "\n",
    "**Import modules**\n",
    "\n",
    "Begin by importing the modules to be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc5b778-fdba-4ce4-b322-aa326262b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f34d2-a5e2-45aa-a931-3aff21a69290",
   "metadata": {},
   "source": [
    "## The MNIST Dataset\n",
    "In this notebook, we will expand on the single layer perceptron introduced in the previous notebook. We will use the same data - the MNIST hand-drawn image data set. Here, let's re-implement the function to read in the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4721c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist_images(data_directory, subset='train'):\n",
    "    if subset=='train':\n",
    "        prefix = 'train-'\n",
    "    else:\n",
    "        prefix = 't10k-'\n",
    "        \n",
    "    with open(os.path.join('MNIST',prefix+'images.idx3-ubyte'), 'rb') as f:\n",
    "        # unpack header\n",
    "        _, num_images, num_rows, num_cols = struct.unpack('>IIII', f.read(16))\n",
    "        \n",
    "        # read image data\n",
    "        image_data = f.read(num_images * num_rows * num_cols)\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\n",
    "        images = images.reshape(num_images, num_rows, num_cols)\n",
    "\n",
    "    with open(os.path.join('MNIST',prefix+'labels.idx1-ubyte'), 'rb') as f:\n",
    "        # unpack header\n",
    "        _, num_labels = struct.unpack('>II', f.read(8))\n",
    "\n",
    "        # read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595de75-c811-4549-a039-6a4c60658a88",
   "metadata": {},
   "source": [
    "Similarly, let's read in our images and reshape them into a stack of \"unraveled\" images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd958e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the training and test images\n",
    "train_images, train_labels = read_mnist_images('MNIST','train')\n",
    "test_images, test_labels = read_mnist_images('MNIST','test')\n",
    "\n",
    "# reshape as before\n",
    "X_train = train_images.reshape(-1, 784) / 255.0\n",
    "X_test = test_images.reshape(-1, 784) / 255.0\n",
    "Y_train = train_labels\n",
    "Y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b7357-a614-47b9-acca-c267704c4e7f",
   "metadata": {},
   "source": [
    "## The Multi-Layer Perception\n",
    "\n",
    "In our previous implementation, we only used one piece of information from each pixel in the prediction of the class probabilities for that pixel. For example, if an image had dark pixels in a vertical line near the center of the image but nowhere else, then that image would yield a high probability for class 1. If there were some additional pixels in the upper left-hand corner of the image, perhaps that would be an elevated probability that the image was a 7. However, sometimes we need to make more complex decisions to account for variations in handwriting. For example, some people write their 4's with a connected tip at the top, and some people keep theirs open. Check out the following example for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd5f565-e1f8-4072-8ded-2ba871841e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_images[58,:,:], cmap='Greys')\n",
    "plt.title('Open 4')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(train_images[150,:,:], cmap='Greys')\n",
    "plt.title('Closed 4')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cbdb0-420d-430f-a1e1-124f0f82fe5a",
   "metadata": {},
   "source": [
    "Clearly, we want our algorithm to work for both of these examples, but these are considerably different shapes. So how can we implement an algorithm to make a choice like this? \n",
    "\n",
    "### Implementing Hidden Layers\n",
    "To implement *non-linear* decisions into a neural network, we can build in **hidden layers** into our model. A hidden layer is simply a layer which is not an input layer and also not an output layer - it's an intermediate layer that is the result of a linear combination of weights and an activation function that is then passed on to another linear combination of weights.\n",
    "\n",
    "To see how these layers work, let's build in a hidden layer to our multi-layer perceptron. In this model, we will have an initial layer of weights $w_1$ and a bias vector $b_1$ that linearly combine to produce $\\zeta$. Then, $\\zeta$ is passed through an activation function $\\sigma$ to give the values in the hidden layer $h$. Then $h$ is passed through a network which is nearly identical to the single-layer perceptron from our previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9d45d-79df-446b-8bc4-af3eaddc9884",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\zeta &= Xw_1^T+b_1 \\\\ \n",
    "h &= \\sigma(\\zeta) \\\\ \n",
    "z &= hw_2^T+b_2 \\\\ \n",
    "p &= \\sigma(z)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c90be1-27cf-4f27-8518-6c92e9bf8a92",
   "metadata": {},
   "source": [
    "To provide a concrete example for the shapes of all matrices in this network, we can step through a miniature example with matrices that have the following sizes:\n",
    "- X: (2, 3)\n",
    "- $w_1$: (5, 3)\n",
    "- $\\zeta$: (2, 5)\n",
    "- $h$: (2, 5)\n",
    "- $w_2$: (4, 5)\n",
    "- z: (2, 4)\n",
    "- p: (2, 4)\n",
    "\n",
    "In the first step, we can see how the weights combine to yiled the contents of the hidden layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b69563-564f-4c74-b722-1d92ff0aa2da",
   "metadata": {},
   "source": [
    "<img src=\"Multilayer_Perceptron_Part2.png\" alt=\"Schematic of a single layer perceptron\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549856c-c6fa-4ff0-87d4-d0b3fcbe0b8a",
   "metadata": {},
   "source": [
    "Then, this result is then passed through an additional set of weights and an additional activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b5b06-fcc8-47ab-aa60-b1c95c4f3579",
   "metadata": {},
   "source": [
    "<img src=\"Multilayer_Perceptron_Part1.png\" alt=\"Schematic of a single layer perceptron\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d1355-348c-40a0-ab09-8988ed10f39b",
   "metadata": {},
   "source": [
    " Note that the output probabilities are the same shape as the output probabilities in the previous single layer perceptron example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213fb50-f89e-4898-9734-7cd62f13f548",
   "metadata": {},
   "source": [
    "#### The Loss Function\n",
    "To compute the optimal weights for our model, we'll need to define a loss function. Here, we will again use the mean square error loss function:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{NC} \\sum_{n=1}^N \\sum_{c=1}^C (y_{n,c}-p_{n,c})^2\n",
    "$$\n",
    "\n",
    "Here, the loss is computed across all of the $N$ images (60,000 in this case) and all of the $C$ classes (10 in this case).\n",
    "\n",
    "#### The Derivative of the Loss Function\n",
    "To use the gradient descent algorithm for each of our weights, we need to compute the derivatives of the loss function with respect to *each* of the weights in *each* of the weight matrices connecting the layers of our model. Here, we'll start by discussing the second weight matrix and then we'll address the first. The reason for this ordering willl become apparent shortly. \n",
    "\n",
    "##### The Second Weight Matrix\n",
    "\n",
    "While we implement our model from the input images through to the output probabilities, we are first going to consider the derivatives of the loss function with respect to the *second* set of weights, $w_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea38c8-4a15-46b7-9909-2aa84fb6233a",
   "metadata": {},
   "source": [
    "If we examine the second schematic figure above, we find that this is identical to the single layer perceptron. Thus, the gradients are identical and are solved by the chain rule yielding:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial w_{2,c,k}}  = \\frac{-1}{N} \\sum_{n=1}^N 2(y_{n,c}-p_{n,c}) p_{n,c}(1-p_{n,c}) h_{n,k}\n",
    "$$\n",
    "\n",
    "This is almost identical to derivative of the loss function with respect to the weights in the single layer perceptron example. However, some notes are helpful for our understanding. First, there is a subscript of 2 on $w$ signifying these are weights of the second linear model. Second, we see that $h_{n,k}$ has replaced $x_{n,i}$ as the second layer output uses output from the first. Other than these details, all of the calculations of the derivatives are the same as the numerical structure of the connections between the hidden layer and output layer are identical to the single layer perceptron. Just as before, we may again concisely write:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{w}_2} = \\begin{bmatrix} \\mathcal{D}_{2,1,1} & \\mathcal{D}_{2,2,1} \\\\ \n",
    "\\mathcal{D}_{2,1,2} & \\mathcal{D}_{2,2,2} \\\\\n",
    "\\mathcal{D}_{2,1,3} & \\mathcal{D}_{2,2,3} \\\\\n",
    "\\mathcal{D}_{2,1,4} & \\mathcal{D}_{2,2,4} \\end{bmatrix} \\begin{bmatrix} h_{1,1} & h_{1,2} & h_{1,3} & h_{1,4} & h_{1,5}\\\\ \n",
    "h_{2,1} & h_{2,2} & h_{2,3} & h_{2,4} & h_{2,5} \\end{bmatrix} = \\mathcal{D_2}^T \\textbf{h}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{2,n,c} = -2(y_{n,c} - p_{n,c}) p_{n,c}(1-p_{n,c})\n",
    "$$\n",
    "\n",
    "\n",
    "##### The First Weight Matrix\n",
    "\n",
    "Now that we've computed the second weight matrix, we can move on to the first. Now, consider the second diagram above and consider the weight $w_{1,5,2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27603223-fe2e-48dd-8be2-e96d05592587",
   "metadata": {},
   "source": [
    "As we can see, the loss function's dependence on weight $w_{1,5,2}$ is a function of *all* the weights that conected the output layer to the first element of the hidden layer. Thus, when computing the derivative, we need to consider all of the different probability values - not just those which correspond to an individual column. Let's write out the full derivative for the weight $w_{1,5,2}$ above:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial w_{1,5,2}}\n",
    "=& \\dfrac{\\partial L}{\\partial p_{1,1}} \\dfrac{\\partial p_{1,1}}{\\partial z_{1,1}} \\dfrac{\\partial z_{1,1}}{\\partial h_{1,5}} \\dfrac{\\partial h_{1,5}}{\\partial \\zeta_{1,5}} \\dfrac{\\partial \\zeta_{1,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{1,2}} \\dfrac{\\partial p_{1,2}}{\\partial z_{1,2}} \\dfrac{\\partial z_{1,2}}{\\partial h_{1,5}} \\dfrac{\\partial h_{1,5}}{\\partial \\zeta_{1,5}} \\dfrac{\\partial \\zeta_{1,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{1,3}} \\dfrac{\\partial p_{1,3}}{\\partial z_{1,3}} \\dfrac{\\partial z_{1,3}}{\\partial h_{1,5}} \\dfrac{\\partial h_{1,5}}{\\partial \\zeta_{1,5}} \\dfrac{\\partial \\zeta_{1,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{1,4}} \\dfrac{\\partial p_{1,4}}{\\partial z_{1,4}} \\dfrac{\\partial z_{1,4}}{\\partial h_{1,5}} \\dfrac{\\partial h_{1,5}}{\\partial \\zeta_{1,5}} \\dfrac{\\partial \\zeta_{1,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{2,1}} \\dfrac{\\partial p_{2,1}}{\\partial z_{2,1}} \\dfrac{\\partial z_{2,1}}{\\partial h_{2,5}} \\dfrac{\\partial h_{2,5}}{\\partial \\zeta_{2,5}} \\dfrac{\\partial \\zeta_{2,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{2,2}} \\dfrac{\\partial p_{2,2}}{\\partial z_{2,2}} \\dfrac{\\partial z_{2,2}}{\\partial h_{2,5}} \\dfrac{\\partial h_{2,5}}{\\partial \\zeta_{2,5}} \\dfrac{\\partial \\zeta_{2,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{2,3}} \\dfrac{\\partial p_{2,3}}{\\partial z_{2,3}} \\dfrac{\\partial z_{2,3}}{\\partial h_{2,5}} \\dfrac{\\partial h_{2,5}}{\\partial \\zeta_{2,5}} \\dfrac{\\partial \\zeta_{2,5}}{\\partial w_{1,5,2}}\\\\\n",
    "&+ \\dfrac{\\partial L}{\\partial p_{2,4}} \\dfrac{\\partial p_{2,4}}{\\partial z_{2,4}} \\dfrac{\\partial z_{2,4}}{\\partial h_{2,5}} \\dfrac{\\partial h_{2,5}}{\\partial \\zeta_{2,5}} \\dfrac{\\partial \\zeta_{2,5}}{\\partial w_{1,5,2}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc1c80-4a21-4a23-9f1e-c3deb36e53cc",
   "metadata": {},
   "source": [
    "Here, we can start filling in a few different components. First, the first two terms are just given by $\\mathcal{D}_{2,n,c}$ (an important observation). Second, the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969f1bf-da0d-42ac-8b91-7d1d4ab85054",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial L}{\\partial w_{1,5,2}}\n",
    "=& \\mathcal{D}_{2,1,1} w_{2,1,5} h_{1,5}(1-h_{1,5}) x_{1,2}\\\\\n",
    "&+ \\mathcal{D}_{2,1,2} w_{2,2,5} h_{1,5}(1-h_{1,5}) x_{1,2}\\\\\n",
    "&+ \\mathcal{D}_{2,1,3} w_{2,3,5} h_{1,5}(1-h_{1,5}) x_{1,2}\\\\\n",
    "&+ \\mathcal{D}_{2,1,4} w_{2,4,5} h_{1,5}(1-h_{1,5}) x_{1,2}\\\\\n",
    "&+ \\mathcal{D}_{2,2,1} w_{2,1,5} h_{2,5}(1-h_{2,5}) x_{2,2}\\\\\n",
    "&+ \\mathcal{D}_{2,2,2} w_{2,2,5} h_{2,5}(1-h_{2,5}) x_{2,2}\\\\\n",
    "&+ \\mathcal{D}_{2,2,3} w_{2,3,5} h_{2,5}(1-h_{1,5}) x_{2,2}\\\\\n",
    "&+ \\mathcal{D}_{2,2,4} w_{2,4,5} h_{2,5}(1-h_{2,5}) x_{2,2}\\\\\n",
    "=&  \\sum_{n=1}^2 \\sum_{c=1}^4 w_{2,c,5} \\mathcal{D}_{2,n,c} h_{n,5}(1-h_{n,5}) x_{n,2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3d1f8-8d47-4cf4-9600-fbad408da999",
   "metadata": {},
   "source": [
    "Here again, we can define a formula $\\mathcal{D}_{1,n,k}$ to make the multiplication more concise:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{1,n,k} = \\sum_{c=1}^4 w_{2,c,k} \\mathcal{D}_{2,n,c} h_{n,k}(1-h_{n,k})\n",
    "$$\n",
    "\n",
    "With this in hand, the full loss gradient can be defined as follows:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{w}_1} = \\begin{bmatrix}\n",
    "\\sum_{n=1}^2 \\mathcal{D}_{1,n,1} x_{n,1} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,1} x_{n,2} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,1} x_{n,3} \\\\\n",
    "\\sum_{n=1}^2 \\mathcal{D}_{1,n,2} x_{n,1} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,2} x_{n,2} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,2} x_{n,3} \\\\\n",
    "\\sum_{n=1}^2 \\mathcal{D}_{1,n,3} x_{n,1} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,3} x_{n,2} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,4} x_{n,3} \\\\\n",
    "\\sum_{n=1}^2 \\mathcal{D}_{1,n,4} x_{n,1} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,4} x_{n,2} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,5} x_{n,3} \\\\\n",
    "\\sum_{n=1}^2 \\mathcal{D}_{1,n,5} x_{n,1} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,5} x_{n,2} & \\sum_{n=1}^2 \\mathcal{D}_{1,n,5} x_{n,3} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82268404-9bcb-46dc-8acd-24cf47ac7afd",
   "metadata": {},
   "source": [
    "As before, we can express this as the multiplication of matrices:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{w}_1} = \\begin{bmatrix} \\mathcal{D}_{1,1,1} & \\mathcal{D}_{1,2,1} \\\\ \n",
    "\\mathcal{D}_{1,1,2} & \\mathcal{D}_{1,2,2} \\\\\n",
    "\\mathcal{D}_{1,1,3} & \\mathcal{D}_{1,2,3} \\\\\n",
    "\\mathcal{D}_{1,1,4} & \\mathcal{D}_{1,2,4} \\\\\n",
    "\\mathcal{D}_{1,1,5} & \\mathcal{D}_{1,2,5}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,3} \\\\ \n",
    "x_{2,1} & x_{2,2} & x_{2,3}  \\end{bmatrix} = \\mathcal{D}_1^T \\textbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9484221-9685-48ee-a19d-cb6e9708fc02",
   "metadata": {},
   "source": [
    "This gives us a recipe to compute the loss gradients - but we also need to compute $\\mathcal{D}_{1}$. Looking back to the definition above, we can see that this can be accomplished with the following computation:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{1} = \\mathcal{D}_{2} \\cdot \\textbf{w}_2  \\textbf{h}(1-\\textbf{h})\n",
    "$$\n",
    "\n",
    "where I have used the $\\cdot$ operator here to indicate matrix multiplication. It is key to note here that all of the products are already computed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73797d80-0666-477a-a7a9-a97253e06b8c",
   "metadata": {},
   "source": [
    "### Algorithmic Summary\n",
    "\n",
    "Ok, that's a lot of details, let's do a quick review before we code it up.\n",
    "\n",
    "#### Forward Model Algorithm\n",
    "To compute the forward model, there are two steps:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textbf{h} &= \\sigma(\\textbf{X} \\textbf{w}_1^T + \\textbf{b}_1 ) \\\\ \n",
    "\\textbf{p} &= \\sigma(\\textbf{h} \\textbf{w}_2^T + \\textbf{b}_2 ) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Not too bad... right?\n",
    "\n",
    "#### Backward Model Algorithm\n",
    "In the backward model, we compute the gradients of the loss functions with respect to the weight matrices. First, we need to compute gradient of the loss function with respect to the second set of wieghts. We can do this short-hand by computing $\\mathcal{D}_{2}$ as \n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{2} = -\\frac{2}{N}(\\textbf{y} - \\textbf{p}) \\textbf{p} (1-\\textbf{p})\n",
    "$$\n",
    "Note that this is done element wise *and we already computed $\\textbf{h}$ and $\\textbf{p}$ in the previous step!* Using this matrix, we get the losses as\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{w}_2} = \\mathcal{D_2}^T \\textbf{h}\n",
    "$$\n",
    "\n",
    "We did not show it here but we can also get the gradient of the loss function with respect to the second bias matrix as \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{b}_2} = \\mathcal{D_2}^T \\textbf{1}\n",
    "$$\n",
    "\n",
    "Next up, we compute the gradient of the loss function with respect to the first set of weights. Again, we can do this short-hand by computing $\\mathcal{D}_{1}$ as \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{D}_{1} = \\mathcal{D}_{2} \\cdot \\textbf{w}_2  \\textbf{h}(1-\\textbf{h})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Using this matrix, we get the losses as\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{w}_1} = \\mathcal{D_1}^T \\textbf{x}\n",
    "$$\n",
    "\n",
    "Again, we can also get the gradient of the loss function with respect to the first bias matrix as \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial \\textbf{b}_1} = \\mathcal{D_1}^T \\textbf{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece015e-9b71-44f1-a385-0b02b37413d4",
   "metadata": {},
   "source": [
    "## Coding Up a Solution\n",
    "Now that we know how to compute our gradients, let's write some code to create our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36bb309-9677-45d0-8d2b-d24f858ada74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        # first layer weights and biases\n",
    "        self.weight_h = rng.normal(loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "\n",
    "        # second layer weights and biases\n",
    "        self.weight_out = rng.normal(loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "\n",
    "    def int_to_onehot(self, y, num_labels):\n",
    "        ary = np.zeros((y.shape[0], num_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            ary[i, val] = 1\n",
    "        return(ary)\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return (1/(1+np.exp(-z)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # enter code here to implement the forward method\n",
    "        zeta = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        h = self.sigmoid(zeta)\n",
    "\n",
    "        z = np.dot(h, self.weight_out.T) + self.bias_out\n",
    "        p = self.sigmoid(z)\n",
    "        return h, p\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def backward(self, x, h, p, y):\n",
    "\n",
    "        # encode the labels with one-hot encoding\n",
    "        y_onehot = self.int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # enter code here to implement the backward method\n",
    "        D2 = (-2 / y.shape[0]) * (y_onehot - p) * p * (1 - p)\n",
    "        d_L__d_w2 = np.dot(D2.T, h)\n",
    "        d_L__d_b2 = np.dot(D2.T, np.ones_like(y))\n",
    "\n",
    "        D1 = np.dot(D2, self.weight_out) * h * (1 - h)\n",
    "        d_L__d_w1 = np.dot(D1.T, x)\n",
    "        d_L__d_b1 = np.dot(D1.T, np.ones_like(y))\n",
    "\n",
    "        return d_L__d_w2, d_L__d_b2, d_L__d_w1, d_L__d_b1\n",
    "\n",
    "    def mse_loss(self, targets, probs, num_labels=10):\n",
    "        onehot_targets = self.int_to_onehot(targets, num_labels)\n",
    "        err = np.mean((onehot_targets - probs)**2)\n",
    "        return(err)\n",
    "\n",
    "    def compute_mse_and_acc(self, X, y, num_labels=10):\n",
    "        _, probs = self.forward(X)\n",
    "        predicted_labels = np.argmax(probs,axis=1)\n",
    "        onehot_targets =self.int_to_onehot(y, num_labels)\n",
    "        loss = np.mean((onehot_targets - probs)**2)\n",
    "        acc = np.sum(predicted_labels == y)/len(y)\n",
    "        return(loss, acc)\n",
    "\n",
    "    def train(self,X_train, Y_train, X_test, Y_test, num_epochs, learning_rate=0.1):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for e in range(num_epochs):\n",
    "\n",
    "            # compute the forward method\n",
    "            h, p = self.forward(X_train)\n",
    "\n",
    "            # compute the backward method\n",
    "            d_L__d_w2, d_L__d_b2, d_L__d_w1, d_L__d_b1 = self.backward(X_train, h, p, Y_train)\n",
    "\n",
    "            # update the weights and the biases\n",
    "            self.weight_out -= learning_rate * d_L__d_w2\n",
    "            self.bias_out -= learning_rate * d_L__d_b2\n",
    "            self.weight_h -= learning_rate * d_L__d_w1\n",
    "            self.bias_h -= learning_rate * d_L__d_b1\n",
    "\n",
    "            train_loss, train_acc = self.compute_mse_and_acc(X_train, Y_train)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "\n",
    "            test_loss, test_acc = self.compute_mse_and_acc(X_test, Y_test)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accs.append(test_acc)\n",
    "        return(train_losses, train_accs, test_losses, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569273d1-009f-48f7-b78b-56a2cc95b1dc",
   "metadata": {},
   "source": [
    "Let's give it a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03ce7a5-4ecc-48ef-b96e-4154a28fee65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# implement your model here\u001b[39;00m\n\u001b[32m      2\u001b[39m mlp = MultiLayerPerceptron(num_features=\u001b[32m784\u001b[39m, num_hidden=\u001b[32m50\u001b[39m, num_classes=\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_losses, train_accs, test_losses, test_accs = \u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mMultiLayerPerceptron.train\u001b[39m\u001b[34m(self, X_train, Y_train, X_test, Y_test, num_epochs, learning_rate)\u001b[39m\n\u001b[32m     76\u001b[39m h, p = \u001b[38;5;28mself\u001b[39m.forward(X_train)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# compute the backward method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m d_L__d_w2, d_L__d_b2, d_L__d_w1, d_L__d_b1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# update the weights and the biases\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.weight_out -= learning_rate * d_L__d_w2\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mMultiLayerPerceptron.backward\u001b[39m\u001b[34m(self, x, h, p, y)\u001b[39m\n\u001b[32m     45\u001b[39m D2 = (-\u001b[32m2\u001b[39m / y.shape[\u001b[32m0\u001b[39m]) * (y_onehot - p) * p * (\u001b[32m1\u001b[39m - p)\n\u001b[32m     46\u001b[39m d_L__d_w2 = np.dot(D2.T, h)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m d_L__d_b2 = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m D1 = np.dot(D2, \u001b[38;5;28mself\u001b[39m.weight_out) * h * (\u001b[32m1\u001b[39m - h)\n\u001b[32m     50\u001b[39m d_L__d_w1 = np.dot(D1.T, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/CS171/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/CS171/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# implement your model here\n",
    "mlp = MultiLayerPerceptron(num_features=784, num_hidden=50, num_classes=10)\n",
    "train_losses, train_accs, test_losses, test_accs = mlp.train(X_train, Y_train, X_test, Y_test, num_epochs=10, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e7a8-b598-4769-9100-7ede9c56f436",
   "metadata": {},
   "source": [
    "Now that we've trained the model, let's take a look at our loss function and accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35899fc-441c-4505-830a-bee44865a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses,label='Train')\n",
    "plt.plot(test_losses,label='Test')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs,label='Train')\n",
    "plt.plot(test_accs,label='Test')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca0450-84dd-4ea6-9a80-b4b90aca977c",
   "metadata": {},
   "source": [
    "Just as for the single layer perceptron, with more than 1000 iterations and a learning rate of 0.5, our model has now started to converge.\n",
    "\n",
    "Let's see how it has works on a collection of random images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a71d7b-f063-4b8e-a6e1-843a6faf51e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37)\n",
    "test_indices = np.random.randint(low=0, high=10000, size=10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for d, index in enumerate(test_indices):\n",
    "    plt.subplot(2,5,d+1)\n",
    "    plt.imshow(train_images[index,:,:],cmap='Greys')\n",
    "    plt.title('True Label: '+str(train_labels[index])+\\\n",
    "              '\\n Predicted Label: '+str(mlp.predict(train_images[index,:,:].reshape(1, 784))[0]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd635a9b-e830-4503-b0ca-9be04c08ba5b",
   "metadata": {},
   "source": [
    "Seems like we're doing pretty good! But how does this compare to the single layer perceptron? I'll see you in the Problem of the Week!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1077af-c975-41e1-b919-b89e75b1bc92",
   "metadata": {},
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "1. A hidden layer provides a machanism for models to make non-linear decisions.\n",
    "2. Implementing a hidden layer into a network introduces one additional matrix stored in memory as well as two additional matrix operations.\n",
    "3. Hidden layers can substantially improve the ability of a model to learn complex features in a data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS171",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
